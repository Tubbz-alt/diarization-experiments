{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps in data preparation\n",
    "\n",
    "1. Load the audio using librosa\n",
    "2. Get the duration using librosa.get_duration\n",
    "3. Calculate each frame width in ms\n",
    "4. Split the audio on VAD (Below 20db is silence)\n",
    "5. For each split calculate mel (180 frames) \n",
    "6. np.transpose the data Ex: (1,40,180) to (180,1,40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log path: /datadrive2/dalon/diarization-experiments/diarization-experiments/Notebooks/full-pipeline.logs\n"
     ]
    }
   ],
   "source": [
    "# All imports\n",
    "import os, sys, logging\n",
    "import datetime, json\n",
    "import time, shutil, pickle\n",
    "import librosa\n",
    "import librosa.display\n",
    "import pysrt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import subprocess\n",
    "\n",
    "from utils import normalize, loss_cal, optim\n",
    "from tensorflow.contrib import rnn\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "from configuration import get_config\n",
    "\n",
    "config = get_config()\n",
    "log_file = os.path.abspath(\"full-pipeline.logs\")\n",
    "logging.basicConfig(\n",
    "    filename=log_file,\n",
    "    level=logging.DEBUG,\n",
    "    format=\"%(asctime)s:%(levelname)s:%(message)s\"\n",
    "    )\n",
    "print(f'Log path: {log_file}')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All configurations below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "videoid = 'GkOn86EtdNQ' #'YdU7fUXDLpI' #'zPFptdATk_s'# 'cKAnHAHBonM' # 'e-Pjs7UyC8I'\n",
    "\n",
    "\n",
    "random_state = 222 # random seed\n",
    "config.N = 64 # Number of speakers per batch\n",
    "config.M = 10 # Number of utterences per speaker\n",
    "config.iteration = 50000000 # Number of iterations to run\n",
    "config.lr = 1e-3\n",
    "config.hidden = 768 # hidden state dimension of lstm\n",
    "config.proj = 256 # projection dimension of lstm\n",
    "config.tisv_frame_min = 50\n",
    "\n",
    "# config.restore = True\n",
    "config.model_num = 46\n",
    "logging.info(f'N={config.N}, M={config.M}')\n",
    "logging.info(f'Model restore: {config.restore}, Model number: {config.model_num}')\n",
    "\n",
    "# Configurations\n",
    "\n",
    "#_____________ Parameters to tune on dev set _______________________\n",
    "# VAD param\n",
    "# Changing to 25, which will give slightly better intervals, 20 gives very short intervals\n",
    "vad_threshold = 25 # threshold for voice activity detection\n",
    "\n",
    "# Segment param\n",
    "acceptable_shortseg_dur = 0.2 # in second\n",
    "#^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "\n",
    "# model parameters\n",
    "model_path = '/datadrive2/dalon/diarization-experiments/diarization-experiments/models/model.ckpt-46' # model save path\n",
    "dataset_path = '/datadrive2/dalon/diarization-experiments/diarization-experiments/audio/'\n",
    "save_dir_path = '/datadrive2/dalon/diarization-experiments/diarization-experiments/embeddings'\n",
    "os.makedirs(save_dir_path, exist_ok=True)\n",
    "audio_file_name = f'{videoid}.wav'\n",
    "output_cluster_path = os.path.join(save_dir_path, f'{videoid}.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm saving only 2 embeddings i.e. first and last tisv_frames for given interval in an audio. So each .npy\n",
    "embedding file will have a shape of (2, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /datadrive2/dalon/diarization-experiments/diarization-experiments/Notebooks/utils.py:100: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "embedded size:  (2, 256)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "batch_size = 2 # Fixing to 2 since we take 2 for each interval #utter_batch.shape[1]\n",
    "verif = tf.placeholder(shape=[None, batch_size, 40], dtype=tf.float32)  # verification batch (time x batch x n_mel)\n",
    "batch = tf.concat([verif,], axis=1)\n",
    "\n",
    "# embedding lstm (3-layer default)\n",
    "with tf.variable_scope(\"lstm\"):\n",
    "    lstm_cells = [tf.contrib.rnn.LSTMCell(num_units=config.hidden, num_proj=config.proj) for i in range(config.num_layer)]\n",
    "    lstm = tf.contrib.rnn.MultiRNNCell(lstm_cells)    # make lstm op and variables\n",
    "    outputs, _ = tf.nn.dynamic_rnn(cell=lstm, inputs=batch, dtype=tf.float32, time_major=True)   # for TI-VS must use dynamic rnn\n",
    "    embedded = outputs[-1]                            # the last ouput is the embedded d-vector\n",
    "    embedded = normalize(embedded)                    # normalize\n",
    "\n",
    "print(\"embedded size: \", embedded.shape)\n",
    "\n",
    "config_tensorflow = tf.ConfigProto(\n",
    "        device_count = {'GPU': 0}\n",
    "    )\n",
    "\n",
    "saver = tf.train.Saver(var_list=tf.global_variables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /datadrive2/dalon/diarization-experiments/diarization-experiments/models/model.ckpt-46\n"
     ]
    }
   ],
   "source": [
    "# Each embedding saved file will have (2, 256)\n",
    "with tf.Session(config=config_tensorflow) as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    saver.restore(sess, model_path)\n",
    "    logging.info(\"loading audio\")\n",
    "    audio_path = os.path.join(dataset_path, audio_file_name)\n",
    "#     audio_file_number = audio_file_name.split('.')[0].split('_')[1]\n",
    "    utter, sr = librosa.core.load(audio_path, sr=config.sr)        # load audio\n",
    "    utter_min_len = (config.tisv_frame_min * config.hop + config.window) * sr    # lower bound of utterance length\n",
    "    # Get the duration\n",
    "    duration = librosa.get_duration(utter, sr)\n",
    "    # Duration of each window\n",
    "    duration_per_frame = (duration / utter.shape[0])\n",
    "    logging.info(f'Duration: {duration}\\nDuration per frame: {duration_per_frame}s\\nMin length of utterance: {utter_min_len * duration_per_frame}s')\n",
    "    tisv_frame_duration_s = utter_min_len * duration_per_frame\n",
    "    intervals = librosa.effects.split(utter, top_db=vad_threshold)         # voice activity detection\n",
    "\n",
    "    all_data = []\n",
    "    logging.info('Converting intervals to embeddings')\n",
    "    selected_intervals_idx = []\n",
    "    for idx, current_interval in enumerate(intervals):\n",
    "        if (current_interval[1]-current_interval[0]) > utter_min_len:\n",
    "            # Save these selected intervals, as shorter ones are ignored\n",
    "            selected_intervals_idx.append(idx)\n",
    "            utterances_spec = []\n",
    "            utter_part = utter[current_interval[0]:current_interval[1]]         # save first and last 160 frames of spectrogram.\n",
    "            S = librosa.core.stft(y=utter_part, n_fft=config.nfft,\n",
    "                                  win_length=int(config.window * sr), hop_length=int(config.hop * sr))\n",
    "            S = np.abs(S) ** 2\n",
    "            mel_basis = librosa.filters.mel(sr=sr, n_fft=config.nfft, n_mels=40)\n",
    "            S = np.log10(np.dot(mel_basis, S) + 1e-6)           # log mel spectrogram of utterances\n",
    "            utterances_spec.append(S[:, :config.tisv_frame])\n",
    "            utterances_spec.append(S[:, -config.tisv_frame:])\n",
    "            utterances_spec = np.array(utterances_spec)\n",
    "            utter_batch = np.transpose(utterances_spec, axes=(2,0,1))     # transpose [frames, batch, n_mels]\n",
    "\n",
    "            data = sess.run(embedded, feed_dict={verif:utter_batch})\n",
    "            all_data.extend(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(all_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spectral clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cossine similarity\n",
    "similarity = np.dot(data, data.T)\n",
    "\n",
    "# squared magnitude of preference vectors (number of occurrences) (diagonals are ai*ai)\n",
    "square_mag = np.diag(similarity)\n",
    "\n",
    "# inverse squared magnitude\n",
    "inv_square_mag = 1 / square_mag\n",
    "\n",
    "\n",
    "# if it doesn't occur, set it's inverse magnitude to zero (instead of inf)\n",
    "inv_square_mag[np.isinf(inv_square_mag)] = 0\n",
    "\n",
    "# inverse of the magnitude\n",
    "inv_mag = np.sqrt(inv_square_mag)\n",
    "\n",
    "# cosine similarity (elementwise multiply by inverse magnitudes)\n",
    "cosine = similarity * inv_mag\n",
    "A =  cosine.T * inv_mag\n",
    "\n",
    "# Fill the diagonals with very large negative value\n",
    "np.fill_diagonal(A, -1000)\n",
    "# Fill the diagonals with the max of each row\n",
    "np.fill_diagonal(A, A.max(axis=1))\n",
    "\n",
    "# final step in cossine sim\n",
    "A = (1-A)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Eigen vectors to pick: 2\n"
     ]
    }
   ],
   "source": [
    "# Gaussian blur\n",
    "sigma = 0.5 # we will select sigma as 0.5\n",
    "A_gau = gaussian_filter(A, sigma)\n",
    "\n",
    "# Thresholding using multiplier = 0.01\n",
    "threshold_multiplier = 0.01\n",
    "A_thresh = A_gau * threshold_multiplier\n",
    "\n",
    "# Symmetrization\n",
    "A_sym = np.maximum(A_thresh, A_thresh.T)\n",
    "\n",
    "# Diffusion\n",
    "A_diffusion = A_sym * A_sym.T\n",
    "\n",
    "# Row-wise matrix Normalization\n",
    "Row_max = A_diffusion.max(axis=1).reshape(1, A_diffusion.shape[0])\n",
    "A_norm = A_diffusion / Row_max.T\n",
    "\n",
    "# Eigen decomposition\n",
    "eigval, eigvec = np.linalg.eig(A_norm)\n",
    "# Since eigen values cannot be negative for Positive semi definite matrix, the numpy returns negative values, converting it to positive\n",
    "eigval = np.abs(eigval)\n",
    "# reordering eigen values\n",
    "sorted_eigval_idx = np.argsort(eigval)[::-1]\n",
    "sorted_eigval = np.sort(eigval)[::-1]\n",
    "\n",
    "# For division according to the equation\n",
    "eigval_shifted = np.roll(sorted_eigval, -1)\n",
    "# Thresholding eigen values because we don't need very low eigan values due to errors\n",
    "eigval_thresh = 0.1\n",
    "sorted_eigval = sorted_eigval[sorted_eigval > eigval_thresh]\n",
    "eigval_shifted = eigval_shifted[:sorted_eigval.shape[0]]\n",
    "\n",
    "# Don't take the first value for calculations, if first value is large, following equation will return k=1, and we want more than one clusters\n",
    "# Get the argmax of the division, since its 0 indexed, add 1\n",
    "k = np.argmax(sorted_eigval[1:]/eigval_shifted[1:]) + 2\n",
    "print(f'Number of Eigen vectors to pick: {k}')\n",
    "\n",
    "# Get the indexes of eigen vectors\n",
    "idexes = sorted_eigval_idx[:k]\n",
    "A_eigvec = eigvec[:, idexes]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.03644929, -0.01817526]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_eigvec[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_eigvec = A_eigvec.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_path = os.path.join(save_dir_path, f'{videoid}.spectral.csv')\n",
    "np.savetxt(embeddings_path, A_eigvec, delimiter='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means offline clustering\n",
    "Like in many diarization systems, we integrated the K-Means clustering algorithm with our system. Specifically, we use K-Means++ for initialization. To determine the number of speakers $k$,  we  use  the  “elbow”  of  the  derivatives  of  conditional  Mean Squared Cosine Distances 1 (MSCD) between each embedding to its cluster centroid: <br>\n",
    "$k = arg max_{\\substack{k \\geq 1}} MSCD(k)$ <br>\n",
    "We define cosine distance as $d(x, y) =(1−cos(x, y))/2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize as sk_normalize\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_clusters = 2\n",
    "\n",
    "A_eigvec_norm = sk_normalize(A_eigvec) # l2 normalized\n",
    "kmeans = KMeans(n_clusters=number_of_clusters, init='k-means++', random_state=random_state)\n",
    "kmeans.fit(A_eigvec)\n",
    "labels = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2626"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a diarization files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use selected intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1313"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(selected_intervals_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  512, 34816])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intervals[selected_intervals_idx[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data = []\n",
    "# with open(output_cluster_path, 'w') as f:\n",
    "for idx, i in enumerate(selected_intervals_idx):\n",
    "    start = str(datetime.timedelta(seconds = intervals[i][0] * duration_per_frame))\n",
    "    end = str(datetime.timedelta(seconds = intervals[i][1] * duration_per_frame))\n",
    "    speaker = labels[idx*2]\n",
    "    if labels[idx*2] != labels[(idx*2)+1]:\n",
    "        speaker = 'OL' # possible overlap\n",
    "    json_data.append({\n",
    "        'start': start,\n",
    "        'end': end,\n",
    "        'speaker': str(speaker)\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_cluster_path, 'w') as f:\n",
    "    json.dump(json_data, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
