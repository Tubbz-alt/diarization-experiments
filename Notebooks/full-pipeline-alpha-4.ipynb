{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete diarization pipeline\n",
    "In this notebook you will find all the steps required for diarization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps in data preparation\n",
    "\n",
    "1. Load the audio using librosa\n",
    "2. Get the duration using librosa.get_duration\n",
    "3. Calculate each frame width in ms\n",
    "4. Split the audio on VAD (Below 20db is silence)\n",
    "5. For each split calculate mel (180 frames) \n",
    "6. np.transpose the data Ex: (1,40,180) to (180,1,40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(M=5, N=4, beta1=0.5, beta2=0.9, comment='', hidden=128, hop=0.01, iteration=100000, loss='softmax', lr=0.01, model_num=6, model_path='./tisv_model', nfft=512, noise_filenum=16, noise_path='/datadrive2/dalon/diarization-experiments/Speaker_Verification/data/VCTK-Corpus/noise', num_layer=3, optim='sgd', proj=64, restore=False, sr=8000, tdsv=False, tdsv_frame=80, test_path='/datadrive2/dalon/diarization-experiments/Speaker_Verification/data/VCTK-Corpus/test', tisv_frame=180, train=False, train_path='/datadrive2/dalon/diarization-experiments/Speaker_Verification/data/VCTK-Corpus/train', window=0.025)\n"
     ]
    }
   ],
   "source": [
    "# All imports\n",
    "import os, sys\n",
    "import datetime\n",
    "import time, shutil\n",
    "import librosa\n",
    "import librosa.display\n",
    "import pysrt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import subprocess\n",
    "\n",
    "from utils import normalize, loss_cal, optim\n",
    "from tensorflow.contrib import rnn\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All configurations below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 222 # random seed\n",
    "\n",
    "# Configurations\n",
    "\n",
    "#_____________ Parameters to tune on dev set _______________________\n",
    "# Model param\n",
    "tisv_frame = 50 # max frame number of utterances of tdsv (lower values suffer)\n",
    "window = 0.025 # 25ms\n",
    "hop = 0.01 # 10ms This is frame level precision we will get\n",
    "\n",
    "# VAD param\n",
    "vad_threshold = 20 # threshold for voice activity detection\n",
    "\n",
    "# Segment param\n",
    "acceptable_shortseg_dur = 0.2 # in second\n",
    "#^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "\n",
    "#sr = 8000 # sampling rate, get this from the audio file # It will be infered from the audio file\n",
    "\n",
    "# pick the nfft atleast twice the size of window(whichs is the input) REF: https://stackoverflow.com/a/18080140/3959965\n",
    "# nfft = 512 # ft kernel size\n",
    "\n",
    "# path to the audio file\n",
    "videoid = 'GkOn86EtdNQ'\n",
    "\n",
    "# version number:\n",
    "version_num = f'-{tisv_frame}-{window}-{hop}-{vad_threshold}-{acceptable_shortseg_dur}'\n",
    "\n",
    "# video_list = [\"zPFptdATk_s\", \"GkOn86EtdNQ\", \"e9TC12UQ8og\", \"wA_oLYTddlA\",\n",
    "#               \"KSW4GAx7828\", \"Co4FdrBZgQs\", \"DvmSwBBRLxw\", \"I6K34CYuKuE\", \n",
    "#               \"ajEoDMdfxFA\", \"EM-2VXMg6VY\", \"NwIL6imI6EU\", \"wFYIafs8ngw\", \n",
    "#               \"cxwmYJ0rUvg\"]\n",
    "audio_path = f'/datadrive2/dalon/diarization-experiments/Speaker_Verification/data/youtube-audio/{videoid}.wav'\n",
    "srt_path = f'/datadrive2/dalon/diarization-experiments/Speaker_Verification/data/srts/original/{videoid}.en.srt'\n",
    "embeddings_path = f'/datadrive2/dalon/diarization-experiments/Speaker_Verification/data/embedding/{videoid}-{version_num}.csv'\n",
    "save_srt_path = f'/datadrive2/dalon/diarization-experiments/Speaker_Verification/data/srts/outputs/{videoid}-{version_num}.en.srt'\n",
    "# audio_path = '/datadrive2/dalon/diarization-experiments/Speaker_Verification/data/manual-truth/zPFptdATk_s-7-130.wav'\n",
    "# audio_path = '/datadrive2/dalon/diarization-experiments/Speaker_Verification/data/manual-truth/e9TC12UQ8og-148-309.wav'\n",
    "# audio_path = '/datadrive2/dalon/diarization-experiments/Speaker_Verification/data/manual-truth/GkOn86EtdNQ-540-830.wav'\n",
    "# audio_path = '/datadrive2/dalon/diarization-experiments/Speaker_Verification/data/manual-truth/wA_oLYTddlA-340-425.wav'\n",
    "\n",
    "# model parameters\n",
    "model_path = '/datadrive2/dalon/diarization-experiments/Speaker_Verification/models'\n",
    "model_num = 9\n",
    "hidden = 768\n",
    "proj = 256\n",
    "num_layer = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "utter, sr = librosa.core.load(audio_path, sr=None)        # load audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration: 2669.7375056689343\n",
      "Duration per frame: 2.2675736961451248e-05s\n",
      "Min length of utterance: 0.525s\n"
     ]
    }
   ],
   "source": [
    "utter_min_len = (tisv_frame * hop + window) * sr    # lower bound of utterance length\n",
    "# Get the duration\n",
    "duration = librosa.get_duration(utter, sr)\n",
    "# Duration of each window\n",
    "duration_per_frame = (duration / utter.shape[0])\n",
    "print(f'Duration: {duration}\\nDuration per frame: {duration_per_frame}s\\nMin length of utterance: {utter_min_len * duration_per_frame}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tisv_frame_duration_s = utter_min_len * duration_per_frame\n",
    "intervals = librosa.effects.split(utter, top_db=vad_threshold)         # voice activity detection\n",
    "intervals_in_s = [[round(block[0] * duration_per_frame, 3), round(block[1] * duration_per_frame, 3)] for block in intervals]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.525"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tisv_frame_duration_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_min_frames = (acceptable_shortseg_dur * utter_min_len) // tisv_frame_duration_s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick the nfft atleast twice the size of window(whichs is the input) REF: https://stackoverflow.com/a/18080140/3959965\n",
    "# ft kernel size\n",
    "nfft = int(window // duration_per_frame) * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nfft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_intervals = []\n",
    "new_intervals_in_s = []\n",
    "# creating intervals list which are greater than the min utterance length\n",
    "# The last segment can still be less that utter_min_len, take care of it when you loop\n",
    "for idx, current_interval in enumerate(intervals):\n",
    "    if (current_interval[1]-current_interval[0]) < utter_min_len:\n",
    "        if not len(new_intervals):\n",
    "            new_intervals.append([current_interval[0], current_interval[1]])\n",
    "            new_intervals_in_s.append([intervals_in_s[idx][0], intervals_in_s[idx][1]])\n",
    "        elif (new_intervals[-1][1] - new_intervals[-1][0]) >= utter_min_len:\n",
    "            new_intervals.append([current_interval[0], current_interval[1]])\n",
    "            new_intervals_in_s.append([intervals_in_s[idx][0], intervals_in_s[idx][1]])\n",
    "        else:\n",
    "            new_intervals[-1][1] = current_interval[1]\n",
    "            new_intervals_in_s[-1][1] = intervals_in_s[idx][1]\n",
    "    else:\n",
    "        new_intervals.append([current_interval[0], current_interval[1]])\n",
    "        new_intervals_in_s.append([intervals_in_s[idx][0], intervals_in_s[idx][1]])\n",
    "#     if 1768 <= idx <= 1790:\n",
    "#         print(new_intervals_in_s[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intervals_in_s[1768:1790]\n",
    "# [632, 633]\n",
    "# [642, 643]\n",
    "# [643, 645]\n",
    "# 633 to 643"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "utterances_spec = []\n",
    "intervals_gt_s = []\n",
    "for idx, current_interval in enumerate(new_intervals):\n",
    "    if (current_interval[1]-current_interval[0]) > utter_min_len:\n",
    "        utter_part = utter[current_interval[0]:current_interval[1]]         # save first and last 180 frames of spectrogram.\n",
    "        S = librosa.core.stft(y=utter_part, n_fft=nfft,\n",
    "                              win_length=int(window * sr), hop_length=int(hop * sr))\n",
    "        S = np.abs(S) ** 2\n",
    "        mel_basis = librosa.filters.mel(sr=sr, n_fft=nfft, n_mels=40)\n",
    "        S = np.log10(np.dot(mel_basis, S) + 1e-6)           # log mel spectrogram of utterances\n",
    "\n",
    "        prev_tisv_frame = 0\n",
    "        prev_start = new_intervals_in_s[idx][0]\n",
    "        for i in range(1, S.shape[1]//tisv_frame + 1):\n",
    "            utterances_spec.append(S[:, prev_tisv_frame:tisv_frame * i])\n",
    "            intervals_gt_s.append([prev_start, prev_start + tisv_frame_duration_s])\n",
    "            prev_start = prev_start + tisv_frame_duration_s\n",
    "            prev_tisv_frame = tisv_frame * i\n",
    "        intervals_in_s[-1][1] = new_intervals_in_s[idx][1]  # Aligning the last interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 3181, 40)\n"
     ]
    }
   ],
   "source": [
    "utterances_spec = np.array(utterances_spec)\n",
    "utter_batch = np.transpose(utterances_spec, axes=(2,0,1))     # transpose [frames, batch, n_mels]\n",
    "print(utter_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedded size:  (3181, 256)\n",
      "model path : /datadrive2/dalon/diarization-experiments/Speaker_Verification/models\n",
      "ckpt file is loaded ! /datadrive2/dalon/diarization-experiments/Speaker_Verification/models/model.ckpt-9\n",
      "INFO:tensorflow:Restoring parameters from /datadrive2/dalon/diarization-experiments/Speaker_Verification/models/model.ckpt-9\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "batch_size = utter_batch.shape[1]\n",
    "verif = tf.placeholder(shape=[None, batch_size, 40], dtype=tf.float32)  # verification batch (time x batch x n_mel)\n",
    "batch = tf.concat([verif,], axis=1)\n",
    "\n",
    "# embedding lstm (3-layer default)\n",
    "with tf.variable_scope(\"lstm\"):\n",
    "    lstm_cells = [tf.contrib.rnn.LSTMCell(num_units=hidden, num_proj=proj) for i in range(num_layer)]\n",
    "    lstm = tf.contrib.rnn.MultiRNNCell(lstm_cells)    # make lstm op and variables\n",
    "    outputs, _ = tf.nn.dynamic_rnn(cell=lstm, inputs=batch, dtype=tf.float32, time_major=True)   # for TI-VS must use dynamic rnn\n",
    "    embedded = outputs[-1]                            # the last ouput is the embedded d-vector\n",
    "    embedded = normalize(embedded)                    # normalize\n",
    "\n",
    "print(\"embedded size: \", embedded.shape)\n",
    "\n",
    "saver = tf.train.Saver(var_list=tf.global_variables())\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"model path :\", model_path)\n",
    "    ckpt = tf.train.get_checkpoint_state(checkpoint_dir=model_path)\n",
    "    ckpt_list = ckpt.all_model_checkpoint_paths\n",
    "    loaded = 0\n",
    "    for model in ckpt_list:\n",
    "        if model_num == int(model[-1]):    # find ckpt file which matches configuration model number\n",
    "            print(\"ckpt file is loaded !\", model)\n",
    "            loaded = 1\n",
    "            saver.restore(sess, model)  # restore variables from selected ckpt file\n",
    "            break\n",
    "    if loaded == 0:\n",
    "        raise AssertionError(\"ckpt file does not exist! Check config.model_num or config.model_path.\")\n",
    "    data = sess.run(embedded, feed_dict={verif:utter_batch})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spectral clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cossine similarity\n",
    "similarity = np.dot(data, data.T)\n",
    "\n",
    "# squared magnitude of preference vectors (number of occurrences) (diagonals are ai*ai)\n",
    "square_mag = np.diag(similarity)\n",
    "\n",
    "# inverse squared magnitude\n",
    "inv_square_mag = 1 / square_mag\n",
    "\n",
    "# if it doesn't occur, set it's inverse magnitude to zero (instead of inf)\n",
    "inv_square_mag[np.isinf(inv_square_mag)] = 0\n",
    "\n",
    "# inverse of the magnitude\n",
    "inv_mag = np.sqrt(inv_square_mag)\n",
    "\n",
    "# cosine similarity (elementwise multiply by inverse magnitudes)\n",
    "cosine = similarity * inv_mag\n",
    "A =  cosine.T * inv_mag\n",
    "\n",
    "# Fill the diagonals with very large negative value\n",
    "np.fill_diagonal(A, -1000)\n",
    "# Fill the diagonals with the max of each row\n",
    "np.fill_diagonal(A, A.max(axis=1))\n",
    "\n",
    "# final step in cossine sim\n",
    "A = (1-A)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Eigen vectors to pick: 2\n"
     ]
    }
   ],
   "source": [
    "# Gaussian blur\n",
    "sigma = 0.5 # we will select sigma as 0.5\n",
    "A_gau = gaussian_filter(A, sigma)\n",
    "\n",
    "# Thresholding using multiplier = 0.01\n",
    "threshold_multiplier = 0.01\n",
    "A_thresh = A_gau * threshold_multiplier\n",
    "\n",
    "# Symmetrization\n",
    "A_sym = np.maximum(A_thresh, A_thresh.T)\n",
    "\n",
    "# Diffusion\n",
    "A_diffusion = A_sym * A_sym.T\n",
    "\n",
    "# Row-wise matrix Normalization\n",
    "Row_max = A_diffusion.max(axis=1).reshape(1, A_diffusion.shape[0])\n",
    "A_norm = A_diffusion / Row_max.T\n",
    "\n",
    "# Eigen decomposition\n",
    "eigval, eigvec = np.linalg.eig(A_norm)\n",
    "# Since eigen values cannot be negative for Positive semi definite matrix, the numpy returns negative values, converting it to positive\n",
    "eigval = np.abs(eigval)\n",
    "# reordering eigen values\n",
    "sorted_eigval_idx = np.argsort(eigval)[::-1]\n",
    "sorted_eigval = np.sort(eigval)[::-1]\n",
    "\n",
    "# For division according to the equation\n",
    "eigval_shifted = np.roll(sorted_eigval, -1)\n",
    "# Thresholding eigen values because we don't need very low eigan values due to errors\n",
    "eigval_thresh = 0.1\n",
    "sorted_eigval = sorted_eigval[sorted_eigval > eigval_thresh]\n",
    "eigval_shifted = eigval_shifted[:sorted_eigval.shape[0]]\n",
    "\n",
    "# Don't take the first value for calculations, if first value is large, following equation will return k=1, and we want more than one clusters\n",
    "# Get the argmax of the division, since its 0 indexed, add 1\n",
    "k = np.argmax(sorted_eigval[1:]/eigval_shifted[1:]) + 2\n",
    "print(f'Number of Eigen vectors to pick: {k}')\n",
    "\n",
    "# Get the indexes of eigen vectors\n",
    "idexes = sorted_eigval_idx[:k]\n",
    "A_eigvec = eigvec[:, idexes]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(embeddings_path, A_eigvec, delimiter='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means offline clustering\n",
    "Like in many diarization systems, we integrated the K-Means clustering algorithm with our system. Specifically, we use K-Means++ for initialization. To determine the number of speakers $k$,  we  use  the  “elbow”  of  the  derivatives  of  conditional  Mean Squared Cosine Distances 1 (MSCD) between each embedding to its cluster centroid: <br>\n",
    "$k = arg max_{\\substack{k \\geq 1}} MSCD(k)$ <br>\n",
    "We define cosine distance as $d(x, y) =(1−cos(x, y))/2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize as sk_normalize\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_clusters = 2\n",
    "\n",
    "A_eigvec_norm = sk_normalize(A_eigvec) # l2 normalized\n",
    "kmeans = KMeans(n_clusters=number_of_clusters, init='k-means++', random_state=random_state)\n",
    "kmeans.fit(A_eigvec)\n",
    "labels = kmeans.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add speakers to the srt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(intervals_gt_s), len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for index, c in enumerate(labels):\n",
    "#     print(f'{datetime.timedelta(seconds=intervals_gt_s[index][0])}=={datetime.timedelta(seconds=intervals_gt_s[index][1])}->{c}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intervals_gt_s.index([635, 636])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "subs = pysrt.open(srt_path, encoding=\"utf-8\")\n",
    "convert_to_s = lambda st: (st.hours * 60 * 60) + \\\n",
    "                            (st.minutes * 60) +\\\n",
    "                            (st.seconds) #+ \\\n",
    "                            #(st.milliseconds / 1000)\n",
    "get_start_and_end = lambda sub: (convert_to_s(sub.start), convert_to_s(sub.end))\n",
    "\n",
    "for sub in subs:\n",
    "    start, end = get_start_and_end(sub)\n",
    "    speakers = []\n",
    "#     speakers_intervals = []\n",
    "    for idx, interval in enumerate(intervals_gt_s):\n",
    "        interval[0], interval[1] = int(interval[0]), int(interval[1])\n",
    "        if interval[0] <= start <= interval[1] or interval[0] <= end <= interval[1]\\\n",
    "            or (start <= interval[0] and interval[1] <= end):\n",
    "            speakers.append(labels[idx])\n",
    "\n",
    "#________________debug________________\n",
    "#     if sub.index == 171:\n",
    "#         for idx, interval in enumerate(intervals_gt_s):\n",
    "#             if interval[0] <= start <= interval[1] or interval[0] <= end <= interval[1]: \n",
    "#                 print(interval)\n",
    "#         print(f'{start, end}')\n",
    "#         print(f'here: {speakers} T:{end - start} {sub.text}')\n",
    "#         break\n",
    "#^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "            \n",
    "    if speakers:\n",
    "#         print(speakers)\n",
    "        sp_list, sp_count = np.unique(speakers, return_counts=True)\n",
    "        speaker_dist = 'Speakers: '\n",
    "        number_speakers = len(speakers)\n",
    "        for idx, sp in enumerate(sp_list):\n",
    "            speaker_dist += f'{sp}, '\n",
    "        sub.text = f'{speaker_dist[:-2]} S:{speakers} T:{end - start} {sub.text}'\n",
    "subs.save(save_srt_path, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start <= 635 and 636 <= end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 635 <= end <= 636"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interval[0], interval[1],start, end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# speakers = []\n",
    "# for idx, interval in enumerate(intervsals_gt_s):\n",
    "#     if interval[0] <= start <= interval[1] or interval[0] <= end <= interval[1]:\n",
    "#         speakers.append(labels[idx])\n",
    "#     if idx == 120: break\n",
    "#     print(speakers, interval[0], interval[1],start, end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count = 0\n",
    "# for i,sub in enumerate(subs):\n",
    "#     if not sub.text.startswith(\"Speakers\"):\n",
    "#         print(sub.index)\n",
    "#         count += 1\n",
    "# print(count, i + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
