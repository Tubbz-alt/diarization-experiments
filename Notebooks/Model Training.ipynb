{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(M=5, N=4, beta1=0.5, beta2=0.9, comment='', hidden=768, hop=0.01, iteration=100000, loss='softmax', lr=0.01, max_batch_utterances=1000, model_num=6, model_path='./tisv_model', nfft=512, noise_filenum=16, noise_path='/datadrive2/dalon/diarization-experiments/Speaker_Verification/data/VCTK-Corpus/noise', num_layer=3, optim='sgd', proj=256, restore=False, sr=8000, tdsv=False, tdsv_frame=80, test_path='/datadrive2/dalon/diarization-experiments/Speaker_Verification/data/VCTK-Corpus/test', tisv_frame=50, train=False, train_path='/datadrive2/dalon/diarization-experiments/Speaker_Verification/data/VCTK-Corpus/train', window=0.025)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "# from utils import normalize, loss_cal, optim\n",
    "from configuration import get_config\n",
    "from tensorflow.contrib import rnn\n",
    "\n",
    "config = get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/datadrive2/dalon/diarization-experiments/Speaker_Verification/tisv-model-voxceleb-1\" # model save path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 2\n"
     ]
    }
   ],
   "source": [
    "if config.M > config.max_batch_utterances:\n",
    "    config.M = config.max_batch_utterances\n",
    "config.M = 100\n",
    "config.N = 2\n",
    "config.iteration = 500000\n",
    "print(config.M, config.N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.train_path = \"/datadrive2/dalon/diarization-experiments/voxceleb-dataset/voxceleb-1/train-data-pruned-1000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def random_batch(speaker_num=config.N, utter_num=config.M, shuffle=True, noise_filenum=None, utter_start=0):\n",
    "    \"\"\" Generate 1 batch.\n",
    "        shuffle : random sampling or not\n",
    "    :return: 1 random numpy batch (frames x batch(NM) x n_mels)\n",
    "    \"\"\"\n",
    "    path = config.train_path\n",
    "    np_file_list = os.listdir(path)\n",
    "    total_speaker = len(np_file_list)\n",
    "\n",
    "    if shuffle:\n",
    "        selected_files = random.sample(np_file_list, speaker_num)  # select random N speakers (default N=4)\n",
    "    else:\n",
    "        selected_files = np_file_list[:speaker_num]                # select first N speakers\n",
    "\n",
    "    utter_batch = []\n",
    "    for file in selected_files:\n",
    "        utters = np.load(os.path.join(path, file))        # load utterance spectrogram of selected speaker\n",
    "        if shuffle:\n",
    "            utter_index = np.random.randint(0, utters.shape[0], utter_num)   # select M utterances per speaker (default M=5)\n",
    "            utter_batch.append(utters[utter_index])       # each speakers utterance [M, n_mels, frames] is appended\n",
    "        else:\n",
    "            utter_batch.append(utters[utter_start: utter_start+utter_num])\n",
    "\n",
    "    utter_batch = np.concatenate(utter_batch, axis=0)     # utterance batch [batch(NM), n_mels, frames]\n",
    "\n",
    "    # for train session, random slicing of input batch\n",
    "    frame_slice = np.random.randint(config.tisv_frame-10, config.tisv_frame-1)\n",
    "    utter_batch = utter_batch[:,:,:frame_slice]\n",
    "\n",
    "    utter_batch = np.transpose(utter_batch, axes=(2,0,1))     # transpose [frames, batch, n_mels]\n",
    "\n",
    "    return utter_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity(embedded, w, b, N=config.N, M=config.M, P=config.proj, center=None):\n",
    "    \"\"\" Calculate similarity matrix from embedded utterance batch (NM x embed_dim) eq. (9)\n",
    "        Input center to test enrollment. (embedded for verification)\n",
    "    :return: tf similarity matrix (NM x N)\n",
    "    \"\"\"\n",
    "    embedded_split = tf.reshape(embedded, shape=[N, M, P])\n",
    "\n",
    "    if center is None:\n",
    "        center = normalize(tf.reduce_mean(embedded_split, axis=1))              # [N,P] normalized center vectors eq.(1)\n",
    "        center_except = normalize(tf.reshape(tf.reduce_sum(embedded_split, axis=1, keep_dims=True)\n",
    "                                             - embedded_split, shape=[N*M,P]))  # [NM,P] center vectors eq.(8)\n",
    "        # make similarity matrix eq.(9)\n",
    "        S = tf.concat(\n",
    "            [tf.concat([tf.reduce_sum(center_except[i*M:(i+1)*M,:]*embedded_split[j,:,:], axis=1, keep_dims=True) if i==j\n",
    "                        else tf.reduce_sum(center[i:(i+1),:]*embedded_split[j,:,:], axis=1, keep_dims=True) for i in range(N)],\n",
    "                       axis=1) for j in range(N)], axis=0)\n",
    "    else :\n",
    "        # If center(enrollment) exist, use it.\n",
    "        S = tf.concat(\n",
    "            [tf.concat([tf.reduce_sum(center[i:(i + 1), :] * embedded_split[j, :, :], axis=1, keep_dims=True) for i\n",
    "                        in range(N)],\n",
    "                       axis=1) for j in range(N)], axis=0)\n",
    "\n",
    "    S = tf.abs(w)*S+b   # rescaling\n",
    "\n",
    "    return S\n",
    "\n",
    "def loss_cal(S, type=\"softmax\", N=config.N, M=config.M):\n",
    "    \"\"\" calculate loss with similarity matrix(S) eq.(6) (7) \n",
    "    :type: \"softmax\" or \"contrast\"\n",
    "    :return: loss\n",
    "    \"\"\"\n",
    "    S_correct = tf.concat([S[i*M:(i+1)*M, i:(i+1)] for i in range(N)], axis=0)  # colored entries in Fig.1\n",
    "\n",
    "    if type == \"softmax\":\n",
    "        total = -tf.reduce_sum(S_correct-tf.log(tf.reduce_sum(tf.exp(S), axis=1, keep_dims=True) + 1e-6))\n",
    "    elif type == \"contrast\":\n",
    "        S_sig = tf.sigmoid(S)\n",
    "        S_sig = tf.concat([tf.concat([0*S_sig[i*M:(i+1)*M, j:(j+1)] if i==j\n",
    "                              else S_sig[i*M:(i+1)*M, j:(j+1)] for j in range(N)], axis=1)\n",
    "                             for i in range(N)], axis=0)\n",
    "        total = tf.reduce_sum(1-tf.sigmoid(S_correct)+tf.reduce_max(S_sig, axis=1, keep_dims=True))\n",
    "    else:\n",
    "        raise AssertionError(\"loss type should be softmax or contrast !\")\n",
    "\n",
    "    return total\n",
    "\n",
    "def normalize(x):\n",
    "    \"\"\" normalize the last dimension vector of the input matrix\n",
    "    :return: normalized input\n",
    "    \"\"\"\n",
    "    return x/tf.sqrt(tf.reduce_sum(x**2, axis=-1, keep_dims=True)+1e-6)\n",
    "\n",
    "def optim(lr):\n",
    "    \"\"\" return optimizer determined by configuration\n",
    "    :return: tf optimizer\n",
    "    \"\"\"\n",
    "    if config.optim == \"sgd\":\n",
    "        return tf.train.GradientDescentOptimizer(lr)\n",
    "    elif config.optim == \"rmsprop\":\n",
    "        return tf.train.RMSPropOptimizer(lr)\n",
    "    elif config.optim == \"adam\":\n",
    "        return tf.train.AdamOptimizer(lr, beta1=config.beta1, beta2=config.beta2)\n",
    "    else:\n",
    "        raise AssertionError(\"Wrong optimizer type!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model init done here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-6-299d716d8f8b>:52: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "embedded size:  (200, 256)\n",
      "similarity matrix size:  (200, 2)\n",
      "total variables : 4654082\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "tf.reset_default_graph()    # reset graph\n",
    "\n",
    "# draw graph\n",
    "batch = tf.placeholder(shape= [None, config.N*config.M, 40], dtype=tf.float32)  # input batch (time x batch x n_mel)\n",
    "lr = tf.placeholder(dtype= tf.float32)  # learning rate\n",
    "global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "w = tf.get_variable(\"w\", initializer= np.array([10], dtype=np.float32))\n",
    "b = tf.get_variable(\"b\", initializer= np.array([-5], dtype=np.float32))\n",
    "\n",
    "# embedding lstm (3-layer default)\n",
    "with tf.variable_scope(\"lstm\"):\n",
    "    lstm_cells = [tf.contrib.rnn.LSTMCell(num_units=config.hidden, num_proj=config.proj) for i in range(config.num_layer)]\n",
    "    lstm = tf.contrib.rnn.MultiRNNCell(lstm_cells)    # define lstm op and variables\n",
    "    outputs, _ = tf.nn.dynamic_rnn(cell=lstm, inputs=batch, dtype=tf.float32, time_major=True)   # for TI-VS must use dynamic rnn\n",
    "    embedded = outputs[-1]                            # the last ouput is the embedded d-vector\n",
    "    embedded = normalize(embedded)                    # normalize\n",
    "print(\"embedded size: \", embedded.shape)\n",
    "\n",
    "# loss\n",
    "sim_matrix = similarity(embedded, w, b)\n",
    "print(\"similarity matrix size: \", sim_matrix.shape)\n",
    "loss = loss_cal(sim_matrix, type=config.loss)\n",
    "\n",
    "# optimizer operation\n",
    "trainable_vars= tf.trainable_variables()                # get variable list\n",
    "optimizer= optim(lr)                                    # get optimizer (type is determined by configuration)\n",
    "grads, vars= zip(*optimizer.compute_gradients(loss))    # compute gradients of variables with respect to loss\n",
    "grads_clip, _ = tf.clip_by_global_norm(grads, 3.0)      # l2 norm clipping by 3\n",
    "grads_rescale= [0.01*grad for grad in grads_clip[:2]] + grads_clip[2:]   # smaller gradient scale for w, b\n",
    "train_op= optimizer.apply_gradients(zip(grads_rescale, vars), global_step= global_step)   # gradient update operation\n",
    "\n",
    "# check variables memory\n",
    "variable_count = np.sum(np.array([np.prod(np.array(v.get_shape().as_list())) for v in trainable_vars]))\n",
    "print(\"total variables :\", variable_count)\n",
    "\n",
    "# record loss\n",
    "loss_summary = tf.summary.scalar(\"loss\", loss)\n",
    "merged = tf.summary.merge_all()\n",
    "saver = tf.train.Saver()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training starts here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(iter : 100) loss: 92.0945\n",
      "(iter : 200) loss: 82.1425\n",
      "(iter : 300) loss: 75.3018\n",
      "(iter : 400) loss: 71.5377\n",
      "(iter : 500) loss: 73.1702\n",
      "(iter : 600) loss: 75.5183\n",
      "(iter : 700) loss: 66.4644\n",
      "(iter : 800) loss: 69.3010\n",
      "(iter : 900) loss: 64.0511\n",
      "(iter : 1000) loss: 70.2378\n",
      "(iter : 1100) loss: 62.5021\n",
      "(iter : 1200) loss: 62.9016\n",
      "(iter : 1300) loss: 60.4980\n",
      "(iter : 1400) loss: 62.4593\n",
      "(iter : 1500) loss: 66.8545\n",
      "(iter : 1600) loss: 57.2523\n",
      "(iter : 1700) loss: 63.3582\n",
      "(iter : 1800) loss: 61.0853\n",
      "(iter : 1900) loss: 58.9453\n",
      "(iter : 2000) loss: 58.7139\n",
      "(iter : 2100) loss: 56.8344\n",
      "(iter : 2200) loss: 61.6813\n",
      "(iter : 2300) loss: 58.7397\n",
      "(iter : 2400) loss: 58.7875\n",
      "(iter : 2500) loss: 62.3622\n",
      "(iter : 2600) loss: 59.1000\n",
      "(iter : 2700) loss: 63.5999\n",
      "(iter : 2800) loss: 63.0497\n",
      "(iter : 2900) loss: 58.8327\n",
      "(iter : 3000) loss: 61.3629\n",
      "(iter : 3100) loss: 57.6862\n",
      "(iter : 3200) loss: 55.7771\n",
      "(iter : 3300) loss: 55.0666\n",
      "(iter : 3400) loss: 59.0346\n",
      "(iter : 3500) loss: 52.8427\n",
      "(iter : 3600) loss: 56.1834\n",
      "(iter : 3700) loss: 57.1691\n",
      "(iter : 3800) loss: 56.4139\n",
      "(iter : 3900) loss: 57.2894\n",
      "(iter : 4000) loss: 50.6461\n",
      "(iter : 4100) loss: 55.9005\n",
      "(iter : 4200) loss: 53.9734\n",
      "(iter : 4300) loss: 51.9137\n",
      "(iter : 4400) loss: 51.9531\n",
      "(iter : 4500) loss: 53.2286\n",
      "(iter : 4600) loss: 55.2901\n",
      "(iter : 4700) loss: 54.4327\n",
      "(iter : 4800) loss: 51.5644\n",
      "(iter : 4900) loss: 49.8356\n",
      "(iter : 5000) loss: 47.6182\n",
      "(iter : 5100) loss: 54.7376\n",
      "(iter : 5200) loss: 56.7083\n",
      "(iter : 5300) loss: 51.9343\n",
      "(iter : 5400) loss: 52.1071\n",
      "(iter : 5500) loss: 51.2319\n",
      "(iter : 5600) loss: 50.6660\n",
      "(iter : 5700) loss: 49.7627\n",
      "(iter : 5800) loss: 57.7596\n",
      "(iter : 5900) loss: 49.0371\n",
      "(iter : 6000) loss: 50.0029\n",
      "(iter : 6100) loss: 52.8697\n",
      "(iter : 6200) loss: 48.5205\n",
      "(iter : 6300) loss: 46.1835\n",
      "(iter : 6400) loss: 48.6094\n",
      "(iter : 6500) loss: 49.2875\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#___________Debug________________\n",
    "config.iteration = 10000\n",
    "#^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "\n",
    "# training session\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    os.makedirs(os.path.join(path, \"Check_Point\"))#, exist_ok=True)  # make folder to save model\n",
    "    os.makedirs(os.path.join(path, \"logs\"), exist_ok=True)          # make folder to save log\n",
    "    writer = tf.summary.FileWriter(os.path.join(path, \"logs\"), sess.graph)\n",
    "    epoch = 0\n",
    "    lr_factor = 1   # lr decay factor ( 1/2 per 10000 iteration)\n",
    "    loss_acc = 0    # accumulated loss ( for running average of loss)\n",
    "\n",
    "    for iter in range(config.iteration):\n",
    "        # run forward and backward propagation and update parameters\n",
    "        _, loss_cur, summary = sess.run([train_op, loss, merged],\n",
    "                              feed_dict={batch: random_batch(), lr: config.lr*lr_factor})\n",
    "\n",
    "        loss_acc += loss_cur    # accumulated loss for each 100 iteration\n",
    "\n",
    "        if iter % 10 == 0:\n",
    "            writer.add_summary(summary, iter)   # write at tensorboard\n",
    "        if (iter+1) % 100 == 0:\n",
    "            print(\"(iter : %d) loss: %.4f\" % ((iter+1),loss_acc/100))\n",
    "            loss_acc = 0                        # reset accumulated loss\n",
    "        if (iter+1) % 50000 == 0: # Don't decay at 10k, decay it at 50k\n",
    "            lr_factor /= 2                      # lr decay\n",
    "            print(\"learning rate is decayed! current lr : \", config.lr*lr_factor)\n",
    "        if (iter+1) % 10000 == 0:\n",
    "            saver.save(sess, os.path.join(path, \"./Check_Point/model.ckpt\"), global_step=iter//10000, max_to_keep=None)\n",
    "            print(\"model is saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
