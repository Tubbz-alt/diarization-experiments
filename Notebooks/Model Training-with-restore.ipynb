{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(M=5, N=4, beta1=0.5, beta2=0.9, comment='', hidden=768, hop=0.01, iteration=100000, loss='softmax', lr=0.001, max_batch_utterances=1000, model_num=3, model_path='./tisv_model', nfft=512, noise_filenum=16, noise_path='/datadrive2/dalon/diarization-experiments/Speaker_Verification/data/VCTK-Corpus/noise', num_layer=3, optim='sgd', proj=256, restore=False, sr=16000, tdsv=False, tdsv_frame=160, test_path='/datadrive2/dalon/diarization-experiments/Speaker_Verification/data/VCTK-Corpus/test', tisv_frame=160, tisv_frame_min=50, train=False, train_path='/datadrive2/dalon/diarization-experiments/Speaker_Verification/data/VCTK-Corpus/train', window=0.025)\n",
      "Log path: /datadrive/dalon/diarization-experiments/Notebooks/model-training-with-restore.logs\n"
     ]
    }
   ],
   "source": [
    "import os, logging\n",
    "import time\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from configuration import get_config\n",
    "from tensorflow.contrib import rnn\n",
    "\n",
    "config = get_config()\n",
    "log_file = os.path.abspath(\"model-training-with-restore.logs\")\n",
    "logging.basicConfig(\n",
    "    filename=log_file,\n",
    "    level=logging.DEBUG,\n",
    "    format=\"%(asctime)s:%(levelname)s:%(message)s\"\n",
    "    )\n",
    "print(f'Log path: {log_file}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/datadrive/dalon/models/m-64-10-768-256\" # model save path\n",
    "config.train_path = \"/datadrive/dalon/diarizer-dataset/vctk-vox1n2-libri-npy\"\n",
    "config.N = 64 # Number of speakers per batch\n",
    "config.M = 10 # Number of utterences per speaker\n",
    "config.iteration = 50000000 # Number of iterations to run\n",
    "config.lr = 1e-3\n",
    "config.hidden = 768 # hidden state dimension of lstm\n",
    "config.proj = 256 # projection dimension of lstm\n",
    "config.restore = True\n",
    "# config.model_num = 6\n",
    "logging.info(f'N={config.N}, M={config.M}')\n",
    "logging.info(f'Model restore: {config.restore}, Model number: {config.model_num}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlobalVar(object):\n",
    "    epoch = 0\n",
    "    dataset_size = 0\n",
    "    start =  0\n",
    "    dataset_file_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def random_batch(speaker_num=config.N, utter_num=config.M, shuffle=True, noise_filenum=None, utter_start=0):\n",
    "    \"\"\" Generate 1 batch.\n",
    "        shuffle : random sampling or not\n",
    "    :return: 1 random numpy batch (frames x batch(NM) x n_mels)\n",
    "    \"\"\"\n",
    "#     print(f'In random')\n",
    "#     print(f'Epoch: {epoch}, start: {start}')\n",
    "    path = config.train_path\n",
    "    if GlobalVar.dataset_size == 0:\n",
    "        GlobalVar.dataset_file_list = os.listdir(path)\n",
    "        random.shuffle(GlobalVar.dataset_file_list)\n",
    "        GlobalVar.dataset_size = len(GlobalVar.dataset_file_list)\n",
    "\n",
    "    selected_files = GlobalVar.dataset_file_list[GlobalVar.start:GlobalVar.start+speaker_num]\n",
    "    GlobalVar.start += speaker_num\n",
    "    if GlobalVar.start + speaker_num >= GlobalVar.dataset_size:\n",
    "        logging.info(f'Epoch {GlobalVar.epoch} completed at {str(datetime.utcnow().isoformat()[:-3])}!')\n",
    "        GlobalVar.epoch += 1\n",
    "        GlobalVar.start = 0\n",
    "#         GlobalVar.dataset_file_list = random.sample(os.listdir(path), GlobalVar.dataset_size)\n",
    "        random.shuffle(GlobalVar.dataset_file_list)\n",
    "#     if shuffle:\n",
    "#         selected_files = random.sample(np_file_list, speaker_num)  # select random N speakers (default N=4)\n",
    "#     else:\n",
    "#         selected_files = np_file_list[:speaker_num]                # select first N speakers\n",
    "\n",
    "    utter_batch = []\n",
    "    for file in selected_files:\n",
    "#         print(file)\n",
    "        utters = np.load(os.path.join(path, file))        # load utterance spectrogram of selected speaker\n",
    "        if shuffle:\n",
    "            utter_index = np.random.randint(0, utters.shape[0], utter_num)   # select M utterances per speaker (default M=5)\n",
    "            utter_batch.append(utters[utter_index])       # each speakers utterance [M, n_mels, frames] is appended\n",
    "        else:\n",
    "            utter_batch.append(utters[utter_start: utter_start+utter_num])\n",
    "\n",
    "    utter_batch = np.concatenate(utter_batch, axis=0)     # utterance batch [batch(NM), n_mels, frames]\n",
    "\n",
    "    # for train session, random slicing of input batch\n",
    "    # to confirm with \"d-vector V3\" we pick ranfom slice\n",
    "    frame_slice = np.random.randint(config.tisv_frame_min, config.tisv_frame)\n",
    "    utter_batch = utter_batch[:,:,:frame_slice]\n",
    "\n",
    "    utter_batch = np.transpose(utter_batch, axes=(2,0,1))     # transpose [frames, batch, n_mels]\n",
    "\n",
    "    return utter_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity(embedded, w, b, N=config.N, M=config.M, P=config.proj, center=None):\n",
    "    \"\"\" Calculate similarity matrix from embedded utterance batch (NM x embed_dim) eq. (9)\n",
    "        Input center to test enrollment. (embedded for verification)\n",
    "    :return: tf similarity matrix (NM x N)\n",
    "    \"\"\"\n",
    "    embedded_split = tf.reshape(embedded, shape=[N, M, P])\n",
    "\n",
    "    if center is None:\n",
    "        center = normalize(tf.reduce_mean(embedded_split, axis=1))              # [N,P] normalized center vectors eq.(1)\n",
    "        center_except = normalize(tf.reshape(tf.reduce_sum(embedded_split, axis=1, keep_dims=True)\n",
    "                                             - embedded_split, shape=[N*M,P]))  # [NM,P] center vectors eq.(8)\n",
    "        # make similarity matrix eq.(9)\n",
    "        S = tf.concat(\n",
    "            [tf.concat([tf.reduce_sum(center_except[i*M:(i+1)*M,:]*embedded_split[j,:,:], axis=1, keep_dims=True) if i==j\n",
    "                        else tf.reduce_sum(center[i:(i+1),:]*embedded_split[j,:,:], axis=1, keep_dims=True) for i in range(N)],\n",
    "                       axis=1) for j in range(N)], axis=0)\n",
    "    else :\n",
    "        # If center(enrollment) exist, use it.\n",
    "        S = tf.concat(\n",
    "            [tf.concat([tf.reduce_sum(center[i:(i + 1), :] * embedded_split[j, :, :], axis=1, keep_dims=True) for i\n",
    "                        in range(N)],\n",
    "                       axis=1) for j in range(N)], axis=0)\n",
    "\n",
    "    S = tf.abs(w)*S+b   # rescaling\n",
    "\n",
    "    return S\n",
    "\n",
    "def loss_cal(S, type=\"softmax\", N=config.N, M=config.M):\n",
    "    \"\"\" calculate loss with similarity matrix(S) eq.(6) (7) \n",
    "    :type: \"softmax\" or \"contrast\"\n",
    "    :return: loss\n",
    "    \"\"\"\n",
    "    S_correct = tf.concat([S[i*M:(i+1)*M, i:(i+1)] for i in range(N)], axis=0)  # colored entries in Fig.1\n",
    "\n",
    "    if type == \"softmax\":\n",
    "        total = -tf.reduce_sum(S_correct-tf.log(tf.reduce_sum(tf.exp(S), axis=1, keep_dims=True) + 1e-6))\n",
    "    elif type == \"contrast\":\n",
    "        S_sig = tf.sigmoid(S)\n",
    "        S_sig = tf.concat([tf.concat([0*S_sig[i*M:(i+1)*M, j:(j+1)] if i==j\n",
    "                              else S_sig[i*M:(i+1)*M, j:(j+1)] for j in range(N)], axis=1)\n",
    "                             for i in range(N)], axis=0)\n",
    "        total = tf.reduce_sum(1-tf.sigmoid(S_correct)+tf.reduce_max(S_sig, axis=1, keep_dims=True))\n",
    "    else:\n",
    "        raise AssertionError(\"loss type should be softmax or contrast !\")\n",
    "\n",
    "    return total\n",
    "\n",
    "def normalize(x):\n",
    "    \"\"\" normalize the last dimension vector of the input matrix\n",
    "    :return: normalized input\n",
    "    \"\"\"\n",
    "    return x/tf.sqrt(tf.reduce_sum(x**2, axis=-1, keep_dims=True)+1e-6)\n",
    "\n",
    "def optim(lr):\n",
    "    \"\"\" return optimizer determined by configuration\n",
    "    :return: tf optimizer\n",
    "    \"\"\"\n",
    "    if config.optim == \"sgd\":\n",
    "        return tf.train.GradientDescentOptimizer(lr)\n",
    "    elif config.optim == \"rmsprop\":\n",
    "        return tf.train.RMSPropOptimizer(lr)\n",
    "    elif config.optim == \"adam\":\n",
    "        return tf.train.AdamOptimizer(lr, beta1=config.beta1, beta2=config.beta2)\n",
    "    else:\n",
    "        raise AssertionError(\"Wrong optimizer type!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model init done here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-5-299d716d8f8b>:52: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()    # reset graph\n",
    "global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "with tf.device('/device:GPU:0'):\n",
    "\n",
    "    # draw graph\n",
    "    batch = tf.placeholder(shape= [None, config.N*config.M, 40], dtype=tf.float32)  # input batch (time x batch x n_mel)\n",
    "    lr = tf.placeholder(dtype= tf.float32)  # learning rate\n",
    "#     global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "    w = tf.get_variable(\"w\", initializer= np.array([10], dtype=np.float32))\n",
    "    b = tf.get_variable(\"b\", initializer= np.array([-5], dtype=np.float32))\n",
    "\n",
    "    # embedding lstm (3-layer default)\n",
    "    with tf.variable_scope(\"lstm\"):\n",
    "        lstm_cells = [tf.contrib.rnn.LSTMCell(num_units=config.hidden, num_proj=config.proj) for i in range(config.num_layer)]\n",
    "        lstm = tf.contrib.rnn.MultiRNNCell(lstm_cells)    # define lstm op and variables\n",
    "        outputs, _ = tf.nn.dynamic_rnn(cell=lstm, inputs=batch, dtype=tf.float32, time_major=True)   # for TI-VS must use dynamic rnn\n",
    "        embedded = outputs[-1]                            # the last ouput is the embedded d-vector\n",
    "        embedded = normalize(embedded)                    # normalize\n",
    "    logging.info(f'embedded size: {embedded.shape}')\n",
    "\n",
    "    # loss\n",
    "    sim_matrix = similarity(embedded, w, b)\n",
    "    logging.info(f\"similarity matrix size: {sim_matrix.shape}\")\n",
    "    loss = loss_cal(sim_matrix, type=config.loss)\n",
    "\n",
    "    # optimizer operation\n",
    "    trainable_vars= tf.trainable_variables()                # get variable list\n",
    "    optimizer= optim(lr)                                    # get optimizer (type is determined by configuration)\n",
    "    grads, vars= zip(*optimizer.compute_gradients(loss))    # compute gradients of variables with respect to loss\n",
    "    grads_clip, _ = tf.clip_by_global_norm(grads, 3.0)      # l2 norm clipping by 3\n",
    "    grads_rescale= [0.01*grad for grad in grads_clip[:2]] + grads_clip[2:]   # smaller gradient scale for w, b\n",
    "    train_op= optimizer.apply_gradients(zip(grads_rescale, vars), global_step= global_step)   # gradient update operation\n",
    "\n",
    "    # check variables memory\n",
    "    variable_count = np.sum(np.array([np.prod(np.array(v.get_shape().as_list())) for v in trainable_vars]))\n",
    "    logging.info(f\"total variables : {variable_count}\")\n",
    "\n",
    "# record loss\n",
    "loss_summary = tf.summary.scalar(\"loss\", loss)\n",
    "merged = tf.summary.merge_all()\n",
    "saver = tf.train.Saver(max_to_keep=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training starts here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model path : /datadrive/dalon/models/m-64-10-768-256/Check_Point\n",
      "INFO:tensorflow:Restoring parameters from /datadrive/dalon/models/m-64-10-768-256/Check_Point/model.ckpt-235\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # %%time\n",
    "\n",
    "    #___________Debug________________\n",
    "    # config.iteration = 100000\n",
    "    #^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "    logging.info(f'Training started at: {str(datetime.utcnow().isoformat()[:-3])}')\n",
    "    config_tensorflow = tf.ConfigProto()\n",
    "    config_tensorflow.gpu_options.allow_growth = True\n",
    "\n",
    "    # training session\n",
    "    with tf.Session(config=config_tensorflow) as sess:\n",
    "        tf.global_variables_initializer().run()\n",
    "        os.makedirs(os.path.join(path, \"Check_Point\"), exist_ok=True)  # make folder to save model\n",
    "        os.makedirs(os.path.join(path, \"logs\"), exist_ok=True)          # make folder to save log\n",
    "        writer = tf.summary.FileWriter(os.path.join(path, \"logs\"), sess.graph)\n",
    "        epoch = 0\n",
    "        lr_factor = 1   # lr decay factor ( 1/2 per 10000 iteration)\n",
    "        loss_acc = 0    # accumulated loss ( for running average of loss)\n",
    "        loaded = 0\n",
    "        \n",
    "        if config.restore and not loaded:\n",
    "            model_path = os.path.join(path, 'Check_Point')\n",
    "            print(\"model path :\", model_path)\n",
    "#             ckpt = tf.train.get_checkpoint_state(checkpoint_dir=model_path)\n",
    "#             ckpt_list = ckpt.all_model_checkpoint_paths\n",
    "#             loaded = 0\n",
    "            ckpt = tf.train.get_checkpoint_state(checkpoint_dir=model_path)\n",
    "            ckpt_list = ckpt.all_model_checkpoint_paths\n",
    "            saver.restore(sess, ckpt_list[-1])\n",
    "            loaded = 1\n",
    "#             for model in ckpt_list:\n",
    "#                 if config.model_num == int(model[-1]):    # find ckpt file which matches configuration model number\n",
    "#                     print(\"ckpt file is loaded !\", model)\n",
    "#                     loaded = 1\n",
    "#                     saver.restore(sess, model)  # restore variables from selected ckpt file\n",
    "#                     break\n",
    "#             if loaded == 0:\n",
    "#                 raise AssertionError(\"ckpt file does not exist! Check config.model_num or config.model_path.\")\n",
    "            \n",
    "        for iter in range(config.iteration):\n",
    "            # run forward and backward propagation and update parameters\n",
    "            _, loss_cur, summary = sess.run([train_op, loss, merged],\n",
    "                                  feed_dict={batch: random_batch(), lr: config.lr*lr_factor})\n",
    "\n",
    "            loss_acc += loss_cur    # accumulated loss for each 100 iteration\n",
    "\n",
    "            if iter % 2 == 0:\n",
    "                writer.add_summary(summary, iter)   # write at tensorboard\n",
    "            if (iter+1) % 2 == 0:\n",
    "                logging.info(\"(epoch : %d) (iter : %d) loss: %.4f\" % (GlobalVar.epoch, (iter+1),loss_acc/2))\n",
    "                loss_acc = 0                        # reset accumulated loss\n",
    "            if (iter+1) % 10000 == 0: # decay at 10k\n",
    "                if config.lr*(lr_factor / 2) < 1e-4:\n",
    "                    logging.info(\"learning rate not decaying : \" + str(config.lr*lr_factor))\n",
    "                else:\n",
    "                    lr_factor /= 2                      # lr decay\n",
    "                    logging.info(\"learning rate is decayed! current lr : \" + str(config.lr*lr_factor))\n",
    "            if (iter+1) % 100 == 0:\n",
    "                saver.save(sess, os.path.join(path, \"Check_Point\", \"model.ckpt\"), global_step=iter//100)\n",
    "                logging.info(\"model is saved!\")\n",
    "    logging.info(f'Training ended at: {str(datetime.utcnow().isoformat()[:-3])}')\n",
    "except Exception as e:\n",
    "    logging.exception(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
