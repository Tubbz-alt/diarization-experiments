{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, shutil, logging, pickle\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log path: /datadrive2/dalon/diarization-experiments/Speaker_Verification/Notebooks/data-prep/data-prep-vctk.log\n"
     ]
    }
   ],
   "source": [
    "dataset_name = 'vctk'\n",
    "log_file = f'data-prep-{dataset_name}.log'\n",
    "data_path = \"/datadrive2/dalon/diarization-experiments/Speaker_Verification/data/VCTK-Corpus/wav48\"\n",
    "save_path = os.path.join(os.path.dirname(data_path), 'cleaned-data')\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "# Structure /aac/id0551/videoid/audio.mp4\n",
    "logging.basicConfig(\n",
    "    filename=log_file,\n",
    "    level=logging.DEBUG,\n",
    "    format=\"%(asctime)s:%(levelname)s:%(message)s\"\n",
    "    )\n",
    "print(f'Log path: {os.path.abspath(log_file)}')\n",
    "logging.info(f'Save path: {save_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if tisv_frame=50, min length of utterance = 525ms\n",
    "tisv_frame = 50 # max frame number of utterances of tdsv (lower values suffer)\n",
    "window = 0.025 # 25ms\n",
    "hop = 0.01 # 10ms This is frame level precision we will get\n",
    "# pick the nfft atleast twice the size of window(whichs is the input) REF: https://stackoverflow.com/a/18080140/3959965\n",
    "# ft kernel size, better to have in pow of 2\n",
    "nfft = 512\n",
    "speaker_list = []\n",
    "\n",
    "counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for speaker in os.listdir(data_path):\n",
    "    save_audio_path = os.path.join(save_path, os.path.join(save_path, speaker + \".npy\"))\n",
    "    utterances_spec = []\n",
    "    for idx, audio_path in enumerate(glob(os.path.join(data_path,speaker + \"/*.wav\"))):\n",
    "        logging.info(f'Processing {audio_path}')\n",
    "        \"\"\" Full preprocess of text independent utterance. The log-mel-spectrogram is saved as numpy file.\n",
    "        Each partial utterance is splitted by voice detection using DB\n",
    "        and all the frames from each partial utterance are saved.\n",
    "        \"\"\"\n",
    "#         save_audio_path = os.path.join(save_path, os.path.splitext(os.path.basename(audio_path))[0] + \".npy\")\n",
    "        try:\n",
    "            # since all other datasets are in 16kHz, downsample these 44kHz to 16kHz\n",
    "            utter, sr = librosa.core.load(audio_path, sr=16000)        # load audio\n",
    "            # Get the duration\n",
    "            duration = librosa.get_duration(utter, sr)\n",
    "            # Duration of each window\n",
    "            duration_per_frame = (duration / utter.shape[0])\n",
    "            utter_min_len = (tisv_frame * hop + window) * sr    # lower bound of utterance length\n",
    "            #     logging.debug(f'Duration: {duration}\\nMin length of utterance: {utter_min_len * duration_per_frame}s')\n",
    "\n",
    "    #         logging.debug(f'Processing: {idx + 1}/{len(all_files)}')\n",
    "#             utterances_spec = []\n",
    "\n",
    "            intervals = librosa.effects.split(utter, top_db=20)         # voice activity detection (Below 20db is considered silence)\n",
    "            for interval in intervals:\n",
    "                if (interval[1]-interval[0]) > utter_min_len:           # If partial utterance is sufficient long,\n",
    "                    utter_part = utter[interval[0]:interval[1]]         # save first and last 180 frames of spectrogram.\n",
    "            #                     logging.debug(f'{int(config.window * sr)},{int(config.hop * sr)}')\n",
    "                    S = librosa.core.stft(y=utter_part, n_fft=nfft,\n",
    "                                          win_length=int(window * sr), hop_length=int(hop * sr))\n",
    "                    S = np.abs(S) ** 2\n",
    "                    mel_basis = librosa.filters.mel(sr=sr, n_fft=nfft, n_mels=40)\n",
    "                    S = np.log10(np.dot(mel_basis, S) + 1e-6)           # log mel spectrogram of utterances\n",
    "\n",
    "                    prev_tisv_frame = 0\n",
    "                    for i in range(1, S.shape[1]//tisv_frame + 1):\n",
    "                        utterances_spec.append(S[:, prev_tisv_frame:tisv_frame * i])\n",
    "                        prev_tisv_frame = tisv_frame * i\n",
    "\n",
    "    #                 utterances_spec.append(S[:, :tisv_frame])    # first 180 frames of partial utterance\n",
    "    #                 utterances_spec.append(S[:, -tisv_frame:])   # last 180 frames of partial utterance\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.exception(e)\n",
    "            logging.info(f'Failed in: {audio_path}')\n",
    "\n",
    "    utterances_spec = np.array(utterances_spec)\n",
    "#         os.makedirs(os.path.dirname(save_audio_path), exist_ok=True)\n",
    "    if not utterances_spec.shape[0] == 0:\n",
    "        logging.debug(utterances_spec.shape)\n",
    "        # this will consists all the utterances for that chapter\n",
    "        speaker_list.append([audio_path, utterances_spec.shape, save_audio_path])\n",
    "        np.save(save_audio_path, utterances_spec)\n",
    "#         break\n",
    "with open(os.path.join(save_path, dataset_name + '.b'), \"wb\") as f:\n",
    "    # save the distribution\n",
    "    logging.info(f'Saving processed audio list to {os.path.join(save_path, dataset_name + \".b\")}')\n",
    "    pickle.dump(speaker_list, f)\n",
    "logging.info(\"Completed!\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['/datadrive2/dalon/diarization-experiments/Speaker_Verification/data/VCTK-Corpus/wav48/p259/p259_384.wav',\n",
       "  (1528, 40, 50),\n",
       "  '/datadrive2/dalon/diarization-experiments/Speaker_Verification/data/VCTK-Corpus/cleaned-data/p259.npy'],\n",
       " ['/datadrive2/dalon/diarization-experiments/Speaker_Verification/data/VCTK-Corpus/wav48/p315/p315_158.wav',\n",
       "  (427, 40, 50),\n",
       "  '/datadrive2/dalon/diarization-experiments/Speaker_Verification/data/VCTK-Corpus/cleaned-data/p315.npy'],\n",
       " ['/datadrive2/dalon/diarization-experiments/Speaker_Verification/data/VCTK-Corpus/wav48/p269/p269_208.wav',\n",
       "  (1256, 40, 50),\n",
       "  '/datadrive2/dalon/diarization-experiments/Speaker_Verification/data/VCTK-Corpus/cleaned-data/p269.npy']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speaker_list[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
