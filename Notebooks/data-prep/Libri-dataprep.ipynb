{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download the 3 varients of Libri dataset\n",
    "1. train-clean-100\n",
    "2. train-clean-360\n",
    "3. train-other-500\n",
    "\n",
    "# Structure of data\n",
    "varient-name/Reader-ID/Chapter-ID/audios*\n",
    "\n",
    "# Cleaning\n",
    "As we are interested in different speakers, keep only 8 utterences, usually 15sec long(each). Try to pick randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, shutil, logging, pickle\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "numer_of_file_to_pick = 150\n",
    "dataset_name = 'librispeech'\n",
    "data_path = \"/datadrive2/dalon/diarization-experiments/librispeech-dataset/LibriSpeech\"\n",
    "save_path = os.path.join(os.path.dirname(data_path), 'cleaned-data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_list_to_process = []\n",
    "for folder in os.listdir(data_path):\n",
    "    # Pick a varient out of three\n",
    "    folder = os.path.join(data_path, folder)\n",
    "    if os.path.isdir(folder):\n",
    "#         print(f'Selected: {folder}')\n",
    "        for reader in os.listdir(folder):\n",
    "            reader = os.path.join(folder, reader)\n",
    "            for chapter in os.listdir(reader):\n",
    "                chapter = os.path.join(reader, chapter)\n",
    "                # pick the files\n",
    "                audio_files = []\n",
    "                for audio_path in glob(os.path.join(chapter, \"*.flac\")):\n",
    "                    audio_files.append(audio_path)\n",
    "                i = np.random.randint(0, len(audio_files), size=numer_of_file_to_pick)\n",
    "                # process only 2 out of these\n",
    "                audio_list_to_process.append([audio_files[x] for x in i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# audio_list_to_process[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "819900"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Process files only in this list audio_list_to_process\n",
    "total = sum([len(x) for x in audio_list_to_process])\n",
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log path: /datadrive2/dalon/diarization-experiments/Speaker_Verification/Notebooks/data-prep/data-prep-librispeech.log\n"
     ]
    }
   ],
   "source": [
    "log_file = 'data-prep-librispeech.log'\n",
    "os.makedirs(save_path)\n",
    "# Structure /aac/id0551/videoid/audio.mp4\n",
    "logging.basicConfig(\n",
    "    filename=log_file,\n",
    "    level=logging.DEBUG,\n",
    "    format=\"%(asctime)s:%(levelname)s:%(message)s\"\n",
    "    )\n",
    "print(f'Log path: {os.path.abspath(log_file)}')\n",
    "logging.info(f'Save path: {save_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-ccefba0cd918>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0;31m#                     logging.debug(f'{int(config.window * sr)},{int(config.hop * sr)}')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                     S = librosa.core.stft(y=utter_part, n_fft=nfft,\n\u001b[0;32m---> 39\u001b[0;31m                                           win_length=int(window * sr), hop_length=int(hop * sr))\n\u001b[0m\u001b[1;32m     40\u001b[0m                     \u001b[0mS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m                     \u001b[0mmel_basis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlibrosa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_fft\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnfft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_mels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/datadrive2/dalon/miniconda2/envs/ti-sv/lib/python3.6/site-packages/librosa/core/spectrum.py\u001b[0m in \u001b[0;36mstft\u001b[0;34m(y, n_fft, hop_length, win_length, window, center, dtype, pad_mode)\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[0;31m# RFFT and Conjugate here to match phase from DPWE code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         stft_matrix[:, bl_s:bl_t] = fft.fft(fft_window *\n\u001b[0;32m--> 184\u001b[0;31m                                             \u001b[0my_frames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbl_s\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbl_t\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m                                             axis=0)[:stft_matrix.shape[0]]\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# if tisv_frame=50, min length of utterance = 525ms\n",
    "tisv_frame = 50 # max frame number of utterances of tdsv (lower values suffer)\n",
    "window = 0.025 # 25ms\n",
    "hop = 0.01 # 10ms This is frame level precision we will get\n",
    "# pick the nfft atleast twice the size of window(whichs is the input) REF: https://stackoverflow.com/a/18080140/3959965\n",
    "# ft kernel size, better to have in pow of 2\n",
    "nfft = 512\n",
    "speaker_list = []\n",
    "\n",
    "counter = 0\n",
    "for speaker in audio_list_to_process:\n",
    "    utterances_spec = []\n",
    "    for idx, audio_path in enumerate(speaker):\n",
    "        counter += 1\n",
    "        logging.info(f'Processing {audio_path} {counter}/{total}')\n",
    "        \"\"\" Full preprocess of text independent utterance. The log-mel-spectrogram is saved as numpy file.\n",
    "        Each partial utterance is splitted by voice detection using DB\n",
    "        and all the frames from each partial utterance are saved.\n",
    "        \"\"\"\n",
    "        save_audio_path = os.path.join(save_path, os.path.splitext(os.path.basename(audio_path))[0] + \".npy\")\n",
    "        try:\n",
    "            utter, sr = librosa.core.load(audio_path, sr=None)        # load audio\n",
    "            # Get the duration\n",
    "            duration = librosa.get_duration(utter, sr)\n",
    "            # Duration of each window\n",
    "            duration_per_frame = (duration / utter.shape[0])\n",
    "            utter_min_len = (tisv_frame * hop + window) * sr    # lower bound of utterance length\n",
    "            #     logging.debug(f'Duration: {duration}\\nMin length of utterance: {utter_min_len * duration_per_frame}s')\n",
    "\n",
    "    #         logging.debug(f'Processing: {idx + 1}/{len(all_files)}')\n",
    "#             utterances_spec = []\n",
    "\n",
    "            intervals = librosa.effects.split(utter, top_db=20)         # voice activity detection (Below 20db is considered silence)\n",
    "            for interval in intervals:\n",
    "                if (interval[1]-interval[0]) > utter_min_len:           # If partial utterance is sufficient long,\n",
    "                    utter_part = utter[interval[0]:interval[1]]         # save first and last 180 frames of spectrogram.\n",
    "            #                     logging.debug(f'{int(config.window * sr)},{int(config.hop * sr)}')\n",
    "                    S = librosa.core.stft(y=utter_part, n_fft=nfft,\n",
    "                                          win_length=int(window * sr), hop_length=int(hop * sr))\n",
    "                    S = np.abs(S) ** 2\n",
    "                    mel_basis = librosa.filters.mel(sr=sr, n_fft=nfft, n_mels=40)\n",
    "                    S = np.log10(np.dot(mel_basis, S) + 1e-6)           # log mel spectrogram of utterances\n",
    "\n",
    "                    prev_tisv_frame = 0\n",
    "                    for i in range(1, S.shape[1]//tisv_frame + 1):\n",
    "                        utterances_spec.append(S[:, prev_tisv_frame:tisv_frame * i])\n",
    "                        prev_tisv_frame = tisv_frame * i\n",
    "\n",
    "    #                 utterances_spec.append(S[:, :tisv_frame])    # first 180 frames of partial utterance\n",
    "    #                 utterances_spec.append(S[:, -tisv_frame:])   # last 180 frames of partial utterance\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.exception(e)\n",
    "            logging.info(f'Failed in: {audio_path}')\n",
    "\n",
    "    utterances_spec = np.array(utterances_spec)\n",
    "#         os.makedirs(os.path.dirname(save_audio_path), exist_ok=True)\n",
    "    if not utterances_spec.shape[0] == 0:\n",
    "        logging.debug(utterances_spec.shape)\n",
    "        # this will consists all the utterances for that chapter\n",
    "        speaker_list.append([audio_path, utterances_spec.shape, save_audio_path])\n",
    "        np.save(save_audio_path, utterances_spec)\n",
    "#         break\n",
    "with open(os.path.join(os.path.dirname(save_path), dataset_name + '.b'), \"wb\") as f:\n",
    "    # save the distribution\n",
    "    logging.info(f'Saving processed audio list to {os.path.join(os.path.dirname(save_path), dataset_name + \".b\")}')\n",
    "    pickle.dump(speaker_list, f)\n",
    "logging.info(\"Completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
