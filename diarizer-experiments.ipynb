{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio processing using librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(M=5, N=4, beta1=0.5, beta2=0.9, comment='', hidden=128, hop=0.01, iteration=100000, loss='softmax', lr=0.01, model_num=6, model_path='./tisv_model', nfft=512, noise_filenum=16, noise_path='/datadrive2/dalon/diarization-experiments/Speaker_Verification/data/VCTK-Corpus/noise', num_layer=3, optim='sgd', proj=64, restore=False, sr=8000, tdsv=False, tdsv_frame=80, test_path='/datadrive2/dalon/diarization-experiments/Speaker_Verification/data/VCTK-Corpus/test', tisv_frame=180, train=False, train_path='/datadrive2/dalon/diarization-experiments/Speaker_Verification/data/VCTK-Corpus/train', window=0.025)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from configuration import get_config\n",
    "from utils import keyword_spot\n",
    "\n",
    "config = get_config()   # get arguments from parser\n",
    "\n",
    "# downloaded dataset path\n",
    "audio_path= '/datadrive2/dalon/diarization-experiments/Speaker_Verification/data/VCTK-Corpus/wav48' # utterance dataset\n",
    "clean_path = '/datadrive2/dalon/diarization-experiments/Speaker_Verification/data/VCTK-Corpus/clean-wav48' # clean dataset\n",
    "#noisy_path = r'C:\\Users\\LG\\Documents\\Deep_learning\\speaker_vertification\\noisy_testset_wav'  # noisy dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_spectrogram_tisv():\n",
    "    \"\"\" Full preprocess of text independent utterance. The log-mel-spectrogram is saved as numpy file.\n",
    "        Each partial utterance is splitted by voice detection using DB\n",
    "        and the first and the last 180 frames from each partial utterance are saved. \n",
    "        Need : utterance data set (VTCK)\n",
    "    \"\"\"\n",
    "    print(\"start text independent utterance feature extraction\")\n",
    "    os.makedirs(config.train_path, exist_ok=True)   # make folder to save train file\n",
    "    os.makedirs(config.test_path, exist_ok=True)    # make folder to save test file\n",
    "\n",
    "    utter_min_len = (config.tisv_frame * config.hop + config.window) * config.sr    # lower bound of utterance length\n",
    "    total_speaker_num = len(os.listdir(audio_path))\n",
    "    train_speaker_num= (total_speaker_num//10)*9            # split total data 90% train and 10% test\n",
    "    print(\"total speaker number : %d\"%total_speaker_num)\n",
    "    print(\"train : %d, test : %d\"%(train_speaker_num, total_speaker_num-train_speaker_num))\n",
    "    for i, folder in enumerate(os.listdir(audio_path)):\n",
    "        if folder in c:\n",
    "            #print(f'Not processing since {folder} is already processed')\n",
    "            continue\n",
    "        speaker_path = os.path.join(audio_path, folder)     # path of each speaker\n",
    "        print(\"%dth speaker processing path:%s...\"%(i,speaker_path))\n",
    "        utterances_spec = []\n",
    "        k=0\n",
    "        for utter_name in os.listdir(speaker_path):\n",
    "            #print(utter_name)\n",
    "            utter_path = os.path.join(speaker_path, utter_name)         # path of each utterance\n",
    "            utter, sr = librosa.core.load(utter_path, config.sr)        # load utterance audio\n",
    "            intervals = librosa.effects.split(utter, top_db=20)         # voice activity detection\n",
    "            for interval in intervals:\n",
    "                if (interval[1]-interval[0]) > utter_min_len:           # If partial utterance is sufficient long,\n",
    "                    utter_part = utter[interval[0]:interval[1]]         # save first and last 180 frames of spectrogram.\n",
    "                    S = librosa.core.stft(y=utter_part, n_fft=config.nfft,\n",
    "                                          win_length=int(config.window * sr), hop_length=int(config.hop * sr))\n",
    "                    S = np.abs(S) ** 2\n",
    "                    mel_basis = librosa.filters.mel(sr=config.sr, n_fft=config.nfft, n_mels=40)\n",
    "                    S = np.log10(np.dot(mel_basis, S) + 1e-6)           # log mel spectrogram of utterances\n",
    "\n",
    "                    utterances_spec.append(S[:, :config.tisv_frame])    # first 180 frames of partial utterance\n",
    "                    utterances_spec.append(S[:, -config.tisv_frame:])   # last 180 frames of partial utterance\n",
    "        print(f'Completed {speaker_path}')\n",
    "        utterances_spec = np.array(utterances_spec)\n",
    "        print(utterances_spec.shape)\n",
    "        if i<train_speaker_num:      # save spectrogram as numpy file\n",
    "            np.save(os.path.join(config.train_path, \"speaker%d.npy\"%(i+64)), utterances_spec)\n",
    "        else:\n",
    "            np.save(os.path.join(config.test_path, \"speaker%d.npy\"%(i-train_speaker_num)), utterances_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "# # Run the preprocessing\n",
    "# save_spectrogram_tisv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start text independent utterance feature extraction\n",
      "total speaker number : 109\n",
      "train : 90, test : 19\n",
      "0th speaker processing path:/datadrive2/dalon/diarization-experiments/Speaker_Verification/data/VCTK-Corpus/wav48/p259...\n",
      "Completed /datadrive2/dalon/diarization-experiments/Speaker_Verification/data/VCTK-Corpus/wav48/p259\n",
      "(4, 40, 180)\n"
     ]
    }
   ],
   "source": [
    "print(\"start text independent utterance feature extraction\")\n",
    "os.makedirs(config.train_path, exist_ok=True)   # make folder to save train file\n",
    "os.makedirs(config.test_path, exist_ok=True)    # make folder to save test file\n",
    "\n",
    "utter_min_len = (config.tisv_frame * config.hop + config.window) * config.sr    # lower bound of utterance length\n",
    "total_speaker_num = len(os.listdir(audio_path))\n",
    "train_speaker_num= (total_speaker_num//10)*9            # split total data 90% train and 10% test\n",
    "print(\"total speaker number : %d\"%total_speaker_num)\n",
    "print(\"train : %d, test : %d\"%(train_speaker_num, total_speaker_num-train_speaker_num))\n",
    "for i, folder in enumerate(os.listdir(audio_path)):\n",
    "    speaker_path = os.path.join(audio_path, folder)     # path of each speaker\n",
    "    print(\"%dth speaker processing path:%s...\"%(i,speaker_path))\n",
    "    utterances_spec = []\n",
    "    k=0\n",
    "    for utter_name in os.listdir(speaker_path):\n",
    "        #print(utter_name)\n",
    "        utter_path = os.path.join(speaker_path, utter_name)         # path of each utterance\n",
    "        utter, sr = librosa.core.load(utter_path, config.sr)        # load utterance audio\n",
    "        intervals = librosa.effects.split(utter, top_db=20)         # voice activity detection\n",
    "        for interval in intervals:\n",
    "            if (interval[1]-interval[0]) > utter_min_len:           # If partial utterance is sufficient long,\n",
    "                utter_part = utter[interval[0]:interval[1]]         # save first and last 180 frames of spectrogram.\n",
    "                S = librosa.core.stft(y=utter_part, n_fft=config.nfft,\n",
    "                                      win_length=int(config.window * sr), hop_length=int(config.hop * sr))\n",
    "                S = np.abs(S) ** 2\n",
    "                mel_basis = librosa.filters.mel(sr=config.sr, n_fft=config.nfft, n_mels=40)\n",
    "                S = np.log10(np.dot(mel_basis, S) + 1e-6)           # log mel spectrogram of utterances\n",
    "\n",
    "                utterances_spec.append(S[:, :config.tisv_frame])    # first 180 frames of partial utterance\n",
    "                utterances_spec.append(S[:, -config.tisv_frame:])   # last 180 frames of partial utterance\n",
    "        break\n",
    "    \n",
    "    print(f'Completed {speaker_path}')\n",
    "    utterances_spec = np.array(utterances_spec)\n",
    "    print(utterances_spec.shape)\n",
    "    break\n",
    "    # if i<train_speaker_num:      # save spectrogram as numpy file\n",
    "    #     np.save(os.path.join(config.train_path, \"speaker%d.npy\"%(i+64)), utterances_spec)\n",
    "    # else:\n",
    "    #     np.save(os.path.join(config.test_path, \"speaker%d.npy\"%(i-train_speaker_num)), utterances_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[11264, 31232],\n",
       "       [34304, 61440]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(66592,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utter.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00127534, 0.00122136, 0.0006043 , ..., 0.00128434, 0.001479  ,\n",
       "       0.00149367], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utter[11264:31232]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.324"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "librosa.get_duration(utter, sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Each frame in ms 1.0405'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f'Each frame in ms {8324/8000}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = librosa.util.example_audio_file()\n",
    "y, sr = librosa.load(filename, sr=100, duration=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(491671,)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61.458875"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "librosa.get_duration(y, sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape[0]/sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/datadrive2/dalon/diarization-experiments/Speaker_Verification/data/VCTK-Corpus/test\"\n",
    "file = \"speaker0.npy\"\n",
    "utters = np.load(os.path.join(path, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(334, 40, 180)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utters.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "utter_batch=[]\n",
    "utter_batch.append(utters[0: 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "utter_batch = np.concatenate(utter_batch, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 40, 180)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utter_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "utter_batch = utter_batch[:,:,:160] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 40, 160)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utter_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(160, 4, 40)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utter_batch = np.transpose(utter_batch, axes=(2,0,1))\n",
    "utter_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-2.96778699, -3.23434766, -4.39895536, ..., -5.99681126,\n",
       "         -5.99729541, -5.99971237],\n",
       "        [-1.68480869, -0.93685481, -1.12681371, ..., -2.13418344,\n",
       "         -2.3309338 , -3.11818273],\n",
       "        [-4.34632155, -4.47610423, -4.65433585, ..., -5.9924307 ,\n",
       "         -5.99650542, -5.99639753],\n",
       "        [-3.33079824, -2.90644971, -3.03378336, ..., -5.92655535,\n",
       "         -5.97386883, -5.98753416]],\n",
       "\n",
       "       [[-2.38258957, -3.13043269, -4.94321763, ..., -5.99693354,\n",
       "         -5.99343982, -5.99907139],\n",
       "        [-1.71488264, -0.87780576, -1.11357438, ..., -2.13701642,\n",
       "         -2.20448508, -2.94353631],\n",
       "        [-3.11931196, -4.3663856 , -5.63164155, ..., -5.99180185,\n",
       "         -5.98991966, -5.99476769],\n",
       "        [-2.50139942, -1.45316429, -1.28221885, ..., -5.99326127,\n",
       "         -5.99495222, -5.99250987]],\n",
       "\n",
       "       [[-2.67323416, -3.69146765, -5.10566556, ..., -5.99502541,\n",
       "         -5.99688395, -5.99914167],\n",
       "        [-1.72387589, -0.82287668, -1.06624966, ..., -2.21298825,\n",
       "         -2.05993619, -2.78441491],\n",
       "        [-3.09152426, -4.17396363, -5.49738845, ..., -5.99456141,\n",
       "         -5.98792387, -5.99694156],\n",
       "        [-1.81636461, -0.97739557, -0.75910799, ..., -5.86940186,\n",
       "         -5.43112353, -5.55485954]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-2.59246568, -3.91870794, -5.1286087 , ..., -4.72402177,\n",
       "         -3.96547903, -4.11393305],\n",
       "        [-1.86479498, -1.0299432 , -1.27157825, ..., -2.73579226,\n",
       "         -2.40528461, -2.72510223],\n",
       "        [-1.64642746, -1.22713307, -0.88218484, ..., -5.52709472,\n",
       "         -5.59000796, -5.80062876],\n",
       "        [-2.18682225, -1.47024626, -1.51027458, ..., -5.2380977 ,\n",
       "         -5.67947394, -5.92275716]],\n",
       "\n",
       "       [[-2.71729046, -3.84985896, -4.4867955 , ..., -4.22352107,\n",
       "         -3.93222902, -3.8633389 ],\n",
       "        [-1.62350655, -0.97740133, -1.09466387, ..., -2.71032947,\n",
       "         -2.46298824, -2.89868271],\n",
       "        [-1.68398049, -1.25127147, -0.91803374, ..., -5.64460421,\n",
       "         -5.70040766, -5.92647986],\n",
       "        [-1.60854393, -1.55956234, -1.50147503, ..., -5.14997191,\n",
       "         -5.77955065, -5.95368921]],\n",
       "\n",
       "       [[-2.62461987, -3.50979228, -4.37910203, ..., -4.4806982 ,\n",
       "         -4.16666661, -4.41831656],\n",
       "        [-1.65596426, -1.11721463, -1.0322866 , ..., -2.75897948,\n",
       "         -2.68927782, -3.21803924],\n",
       "        [-1.77959199, -1.29330285, -0.94313819, ..., -5.56039635,\n",
       "         -5.47911547, -5.88844565],\n",
       "        [-1.94890848, -1.20730569, -1.36696863, ..., -5.43433301,\n",
       "         -5.49779344, -5.89926112]]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utter_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
